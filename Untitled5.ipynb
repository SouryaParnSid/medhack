{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47fe590-de1a-425b-87ba-82330ee050f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import classification_report , confusion_matrix , accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow\n",
    "from PIL import Image \n",
    "import tensorflow as tfl\n",
    "from keras.layers import Input, Dense,Conv2D , MaxPooling2D, Flatten,BatchNormalization,Dropout\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import tensorflow_hub as hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745ac3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in c:\\users\\sourya sarkar\\appdata\\roaming\\python\\python313\\site-packages (1.7.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\sourya sarkar\\appdata\\roaming\\python\\python313\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\sourya sarkar\\appdata\\roaming\\python\\python313\\site-packages (from xgboost) (1.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0894ca0a-3c4a-4679-beda-ac79772a8b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_path = \"Brain_Data_Organised/Normal\"  # Use forward slashes for better compatibility\n",
    "stroke_path = \"Brain_Data_Organised/Stroke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "688eae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp310-cp310-win_amd64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.0-cp310-cp310-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\anaconda\\envs\\tfgpu\\lib\\site-packages (from torch) (4.11.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\envs\\tfgpu\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\envs\\tfgpu\\lib\\site-packages (from torch) (3.1.4)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\tfgpu\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\anaconda\\envs\\tfgpu\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\envs\\tfgpu\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.7.0-cp310-cp310-win_amd64.whl (212.5 MB)\n",
      "   ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/212.5 MB 12.6 MB/s eta 0:00:17\n",
      "    --------------------------------------- 4.2/212.5 MB 12.0 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 6.6/212.5 MB 11.5 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 9.4/212.5 MB 12.0 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 12.6/212.5 MB 12.7 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 12.8/212.5 MB 12.6 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 15.7/212.5 MB 11.2 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 17.6/212.5 MB 11.1 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 19.9/212.5 MB 11.0 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 22.0/212.5 MB 11.0 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 24.4/212.5 MB 11.0 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 27.0/212.5 MB 11.0 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 29.4/212.5 MB 11.2 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 31.7/212.5 MB 11.2 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 34.3/212.5 MB 11.2 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 37.2/212.5 MB 11.5 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 40.1/212.5 MB 11.6 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 43.5/212.5 MB 11.9 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 46.7/212.5 MB 12.0 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 48.8/212.5 MB 12.0 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 51.1/212.5 MB 12.0 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 51.6/212.5 MB 11.9 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 54.0/212.5 MB 11.5 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 56.9/212.5 MB 11.7 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 59.8/212.5 MB 11.8 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 62.4/212.5 MB 11.8 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 65.0/212.5 MB 11.9 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 65.0/212.5 MB 11.9 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 67.6/212.5 MB 11.4 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 69.7/212.5 MB 11.4 MB/s eta 0:00:13\n",
      "   ------------- -------------------------- 72.1/212.5 MB 11.5 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 74.7/212.5 MB 11.5 MB/s eta 0:00:13\n",
      "   -------------- ------------------------- 77.3/212.5 MB 11.4 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 80.2/212.5 MB 11.5 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 83.1/212.5 MB 11.6 MB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 85.7/212.5 MB 11.6 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 88.1/212.5 MB 11.6 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 89.9/212.5 MB 11.6 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 90.7/212.5 MB 11.4 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 93.3/212.5 MB 11.4 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 95.9/212.5 MB 11.4 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 99.1/212.5 MB 11.5 MB/s eta 0:00:10\n",
      "   ------------------ -------------------- 102.0/212.5 MB 11.6 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 105.1/212.5 MB 11.7 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 106.7/212.5 MB 11.7 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 108.0/212.5 MB 11.5 MB/s eta 0:00:10\n",
      "   -------------------- ------------------ 110.1/212.5 MB 11.4 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 112.5/212.5 MB 11.4 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 113.5/212.5 MB 11.4 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 115.9/212.5 MB 11.3 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 118.8/212.5 MB 11.4 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 121.6/212.5 MB 11.4 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 124.5/212.5 MB 11.4 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 127.1/212.5 MB 11.5 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 130.0/212.5 MB 11.5 MB/s eta 0:00:08\n",
      "   ------------------------ -------------- 132.9/212.5 MB 11.6 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 136.1/212.5 MB 11.6 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 138.7/212.5 MB 11.6 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 141.8/212.5 MB 11.7 MB/s eta 0:00:07\n",
      "   -------------------------- ------------ 142.6/212.5 MB 11.7 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 145.0/212.5 MB 11.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 145.8/212.5 MB 11.5 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 147.8/212.5 MB 11.4 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 150.7/212.5 MB 11.5 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 152.0/212.5 MB 11.5 MB/s eta 0:00:06\n",
      "   --------------------------- ----------- 152.0/212.5 MB 11.5 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 155.5/212.5 MB 11.3 MB/s eta 0:00:06\n",
      "   ---------------------------- ---------- 157.8/212.5 MB 11.3 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 158.6/212.5 MB 11.3 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 159.6/212.5 MB 11.1 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 160.4/212.5 MB 11.1 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 162.5/212.5 MB 11.0 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 164.9/212.5 MB 11.0 MB/s eta 0:00:05\n",
      "   ------------------------------ -------- 167.8/212.5 MB 11.1 MB/s eta 0:00:05\n",
      "   ------------------------------- ------- 169.9/212.5 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 171.2/212.5 MB 11.0 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 173.5/212.5 MB 11.0 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 176.2/212.5 MB 11.0 MB/s eta 0:00:04\n",
      "   -------------------------------- ------ 178.5/212.5 MB 11.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ----- 181.4/212.5 MB 11.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 182.7/212.5 MB 11.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 184.8/212.5 MB 11.0 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 187.2/212.5 MB 11.0 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 190.3/212.5 MB 11.0 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 192.9/212.5 MB 11.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 194.5/212.5 MB 11.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 195.0/212.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 197.4/212.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 200.3/212.5 MB 11.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 203.2/212.5 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 206.3/212.5 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  207.4/212.5 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  210.5/212.5 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  212.3/212.5 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 212.5/212.5 MB 10.9 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.22.0-cp310-cp310-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 11.6 MB/s eta 0:00:00\n",
      "Downloading torchaudio-2.7.0-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 13.0 MB/s eta 0:00:00\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, fsspec, filelock, torch, torchvision, torchaudio\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.2 mpmath-1.3.0 sympy-1.14.0 torch-2.7.0 torchaudio-2.7.0 torchvision-0.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'd:\\anaconda\\envs\\tfgpu\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts torchfrtrace.exe and torchrun.exe are installed in 'd:\\anaconda\\envs\\tfgpu\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    " pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0213de43-c6aa-4b51-9af6-4135c4f26ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Get filenames from each folder separately:\n",
    "normal_files = os.listdir(os.path.join(\"Brain_Data_Organised\", \"Normal\"))\n",
    "stroke_files = os.listdir(os.path.join(\"Brain_Data_Organised\", \"Stroke\"))\n",
    "\n",
    "data = []\n",
    "\n",
    "# Process normal images\n",
    "for img_file in normal_files:\n",
    "    image_path = os.path.join(\"Brain_Data_Organised\", \"Normal\", img_file)\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((224, 224))\n",
    "    image = image.convert('RGB')\n",
    "    data.append(image)\n",
    "\n",
    "# Process stroke images\n",
    "for img_file in stroke_files:\n",
    "    image_path = os.path.join(\"Brain_Data_Organised\", \"Stroke\", img_file)\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((224, 224))\n",
    "    image = image.convert('RGB')\n",
    "    data.append(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec630dc-9892-4a29-93d1-d67bdaaca960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1551\n",
      "2501\n"
     ]
    }
   ],
   "source": [
    "normal_label = [0]*len(normal_files)\n",
    "stroke_label = [1]*len(stroke_files)\n",
    "Target_label = normal_label + stroke_label\n",
    "print(len(normal_label))\n",
    "print(len(Target_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e64644ad-093b-4396-904a-a0191f2141db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# import albumentations as A\n",
    "# from albumentations.core.composition import OneOf\n",
    "# from albumentations.pytorch import ToTensorV2 \n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "195c9730-1115-412c-a6c9-7e21cc9805d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ðŸš€ 1. Preprocess Data\n",
    "# =============================\n",
    "\n",
    "# Convert data & labels to numpy arrays\n",
    "x = np.array(data)\n",
    "y = np.array(Target_label)\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=42)\n",
    "\n",
    "# Normalize images\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding)\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# y_train = to_categorical(y_train, num_classes=2)\n",
    "# y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "# Data Augmentation\n",
    "# Custom augmentation strategy\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,  # Controlled rotation to preserve diagnostic features\n",
    "    width_shift_range=0.05,  # Minor shifts to avoid cropping key features\n",
    "    height_shift_range=0.05,\n",
    "    zoom_range=0.08,  # Slight zoom, but not too aggressive\n",
    "    brightness_range=[0.85, 1.15],  # Keep contrast variation within realistic limits\n",
    "    horizontal_flip=False,  # Never flip medical images horizontally\n",
    "    fill_mode=\"reflect\",  # Avoid artificial border effects\n",
    "    shear_range=0.02,  # Very minimal shearing to preserve structure\n",
    "    channel_shift_range=0.1  # Subtle intensity variations similar to different CT machines\n",
    ")\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3313b1a-d8ff-472a-a0cc-c04b68d8f845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 100)     2800      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 100)    0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 80)      72080     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 80)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 64)        46144     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 43264)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               11075840  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,229,889\n",
      "Trainable params: 11,229,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# ðŸ“Œ 2. Define CNN Model\n",
    "# =============================\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Activation\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=100,kernel_size=(3,3),strides=(1,1),padding=\"valid\",activation=\"relu\", input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=80,kernel_size=(3,3),strides=(1,1),padding=\"valid\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),strides=(1,1),padding=\"valid\",activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=256, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "# Build CNN model\n",
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11c1d15a-0719-4758-b86e-f398d0dc51ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29d84cb3-93f2-4343-a375-607647206ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('cpu_compiler', 'C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.27.29110/bin/HostX64/x64/cl.exe'), ('cuda_compute_capabilities', ['sm_35', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'compute_80']), ('cuda_version', '64_112'), ('cudart_dll_name', 'cudart64_112.dll'), ('cudnn_dll_name', 'cudnn64_8.dll'), ('cudnn_version', '64_8'), ('is_cuda_build', True), ('is_rocm_build', False), ('is_tensorrt_build', False), ('msvcp_dll_names', 'msvcp140.dll,msvcp140_1.dll'), ('nvcuda_dll_name', 'nvcuda.dll')])\n"
     ]
    }
   ],
   "source": [
    "print(tf.sysconfig.get_build_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c74d36a-5cbf-438e-9ae5-72bc35b7eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# ðŸ”¥ 3. Load EfficientNet Model\n",
    "# =============================\n",
    "\n",
    "efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "efficientnet = tf.keras.Model(inputs=efficientnet.input, outputs=GlobalAveragePooling2D()(efficientnet.output))\n",
    "\n",
    "def extract_features(model, data):\n",
    "    features = model.predict(data,  batch_size=32)\n",
    "    return features.reshape(features.shape[0], -1)  # Flatten features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b1c6785-a329-4aa7-834a-4ddd36ec24c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "71/71 [==============================] - 115s 2s/step - loss: 3.6579 - accuracy: 0.6040 - val_loss: 0.6891 - val_accuracy: 0.5936 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "71/71 [==============================] - 111s 2s/step - loss: 0.6480 - accuracy: 0.6489 - val_loss: 0.6876 - val_accuracy: 0.5936 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "71/71 [==============================] - 109s 2s/step - loss: 0.6343 - accuracy: 0.6476 - val_loss: 0.6861 - val_accuracy: 0.5936 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "71/71 [==============================] - 107s 2s/step - loss: 0.6308 - accuracy: 0.6556 - val_loss: 0.6846 - val_accuracy: 0.5936 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "71/71 [==============================] - 105s 1s/step - loss: 0.6030 - accuracy: 0.6782 - val_loss: 0.6843 - val_accuracy: 0.5936 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "71/71 [==============================] - 104s 1s/step - loss: 0.6009 - accuracy: 0.6862 - val_loss: 0.6845 - val_accuracy: 0.5936 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "71/71 [==============================] - 105s 1s/step - loss: 0.5522 - accuracy: 0.7120 - val_loss: 0.6848 - val_accuracy: 0.5936 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "71/71 [==============================] - ETA: 0s - loss: 0.5426 - accuracy: 0.7227Restoring model weights from the end of the best epoch: 5.\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "71/71 [==============================] - 103s 1s/step - loss: 0.5426 - accuracy: 0.7227 - val_loss: 0.6860 - val_accuracy: 0.5936 - lr: 0.0010\n",
      "Epoch 8: early stopping\n",
      "8/8 [==============================] - 3s 335ms/step - loss: 0.6843 - accuracy: 0.5936\n",
      "Loss on Test Data: 0.6843377947807312\n",
      "Accuracy on Test Data: 0.5936254858970642\n",
      "71/71 [==============================] - 25s 307ms/step - loss: 0.6811 - accuracy: 0.6231\n",
      "Loss on Train Data: 0.6810859441757202\n",
      "Accuracy on Train Data: 0.6231111288070679\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# ðŸ”„ 4. Train Both Models & Extract Features\n",
    "# =============================\n",
    "\n",
    "# Define Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True,verbose=1)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=3)\n",
    "\n",
    "# Train CNN model\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=32),\n",
    "                        epochs=50,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "# Extract CNN Features\n",
    "cnn_features_train = extract_features(model, datagen.flow(x_train))\n",
    "cnn_features_test = extract_features(model, x_test)\n",
    "\n",
    "# Extract EfficientNet Features\n",
    "efficientnet_features_train = extract_features(efficientnet,  datagen.flow(x_train))\n",
    "efficientnet_features_test = extract_features(efficientnet, x_test)\n",
    "\n",
    "# Concatenate CNN & EfficientNet features\n",
    "X_train_meta = np.hstack((cnn_features_train, efficientnet_features_train))\n",
    "X_test_meta = np.hstack((cnn_features_test, efficientnet_features_test))\n",
    "\n",
    "#Convert one-hot labels to single-class labels\n",
    "# y_train_meta = np.argmax(y_train, axis=1)\n",
    "# y_test_meta = np.argmax(y_test, axis=1)\n",
    "y_train_meta = y_train\n",
    "y_test_meta = y_test\n",
    "\n",
    "loss, acc = model.evaluate(x_test,y_test)\n",
    "print(\"Loss on Test Data:\",loss)\n",
    "print(\"Accuracy on Test Data:\",acc)\n",
    "\n",
    "loss, acc = model.evaluate(x_train,y_train)\n",
    "print(\"Loss on Train Data:\",loss)\n",
    "print(\"Accuracy on Train Data:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91c83290-faad-4360-b8b7-119f2e8e0266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "ðŸ† Best XGBoost Hyperparameters: {'subsample': 0.6, 'n_estimators': 150, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.01, 'gamma': 0.1, 'colsample_bytree': 0.6}\n",
      "ðŸŽ¯ Tuned Ensemble Model Accuracy: 59.36%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================\n",
    "# ðŸ† 5. Hyperparameter Tuning with RandomizedSearchCV for XGBoost\n",
    "# =============================\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', tree_method='gpu_hist')\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'gamma': [0, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "# Perform Randomized Search\n",
    "random_search = RandomizedSearchCV(xgb_model, param_distributions=param_dist, cv=5, scoring='accuracy', verbose=3, n_jobs=4, n_iter=10, random_state=42)\n",
    "random_search.fit(X_train_meta, y_train_meta)\n",
    "\n",
    "# Best XGBoost model\n",
    "best_xgb = random_search.best_estimator_\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = best_xgb.predict(X_test_meta)\n",
    "accuracy = accuracy_score(y_test_meta, y_pred)\n",
    "\n",
    "print(f\"ðŸ† Best XGBoost Hyperparameters: {random_search.best_params_}\")\n",
    "print(f\"ðŸŽ¯ Tuned Ensemble Model Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7139218c-403b-4823-b552-04eecb6d418e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# ðŸ“Š 6. Performance Metrics\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, roc_curve, auc\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ðŸŽ¯ 6. Evaluate Model Performance\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Compute accuracy\u001b[39;00m\n\u001b[0;32m     12\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_meta, y_pred)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# ðŸ“Š 6. Performance Metrics\n",
    "# =============================\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns\n",
    "\n",
    "# =============================\n",
    "# ðŸŽ¯ 6. Evaluate Model Performance\n",
    "# =============================\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test_meta, y_pred)\n",
    "print(f\"ðŸŽ¯ Tuned Ensemble Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Classification Report (Precision, Recall, F1-score)\n",
    "print(\"\\nðŸ“„ Classification Report:\\n\")\n",
    "print(classification_report(y_test_meta, y_pred, target_names=[\"Normal\", \"Stroke\"]))\n",
    "\n",
    "# =============================\n",
    "# ðŸ“Œ 7. Confusion Matrix\n",
    "# =============================\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(confusion_matrix(y_test_meta, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Normal\", \"Stroke\"], yticklabels=[\"Normal\", \"Stroke\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# =============================\n",
    "# ðŸ“ˆ 8. ROC Curve (AUC-ROC)\n",
    "# =============================\n",
    "\n",
    "# Get probability scores for ROC Curve\n",
    "y_pred_probs = best_xgb.predict_proba(X_test_meta)[:, 1]  # Probabilities for positive class (Stroke)\n",
    "\n",
    "# Compute ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test_meta, y_pred_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f\"AUC = {roc_auc:.2f}\")\n",
    "plt.plot([0,1], [0,1], 'r--')  # Diagonal line (random classifier)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot Loss Function\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Function Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Assuming you have trained your CNN and have a 'history' object from model.fit()\n",
    "# e.g., history = model.fit(...)\n",
    "\n",
    "# Extract accuracy values from the history object\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']  # If you used a validation set\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, accuracy, 'bo-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'r*-', label='Validation Accuracy')\n",
    "plt.title('Epoch vs. Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(visible=True, linestyle='--', alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02335b61-674b-41e7-884d-6027955a933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Function to visualize images with actual & predicted labels\n",
    "def visualize_predictions(images, actual_labels, predicted_labels, num_images=10, title=\"Predictions\"):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        idx = random.randint(0, len(images) - 1)  # Randomly select an image\n",
    "\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(images[idx])  # Show the image\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Set title with actual & predicted labels\n",
    "        actual = \"Stroke\" if actual_labels[idx] == 1 else \"No Stroke\"\n",
    "        predicted = \"Stroke\" if predicted_labels[idx] == 1 else \"No Stroke\"\n",
    "        \n",
    "        title_color = \"green\" if actual == predicted else \"red\"  # Green = correct, Red = wrong\n",
    "        plt.title(f\"Actual: {actual}\\nPred: {predicted}\", color=title_color)\n",
    "\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get predictions for test images\n",
    "y_test_pred = best_xgb.predict(X_test_meta)\n",
    "\n",
    "# Visualize train images\n",
    "visualize_predictions(x_train, y_train, best_xgb.predict(X_train_meta), title=\"Train Set Predictions\")\n",
    "\n",
    "# Visualize test images\n",
    "visualize_predictions(x_test, y_test, y_test_pred, title=\"Test Set Predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb1a4392-2191-4242-aad4-2b9c7b820fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model saved as best_xgb_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model\n",
    "joblib.dump(best_xgb, 'best_xgb_model.pkl')\n",
    "print(\"âœ… Model saved as best_xgb_model.pkl\")\n",
    "\n",
    "# To load later:\n",
    "# best_xgb = joblib.load('best_xgb_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c708c24-0bd2-4528-861e-7758039ab91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\tfgpu\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\envs\\tfgpu\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.0.1-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.6/150.0 MB 12.0 MB/s eta 0:00:13\n",
      "   - -------------------------------------- 4.5/150.0 MB 14.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 7.6/150.0 MB 14.7 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 11.0/150.0 MB 14.3 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 13.1/150.0 MB 14.2 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 14.2/150.0 MB 12.2 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 16.0/150.0 MB 12.0 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 16.0/150.0 MB 12.0 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 19.9/150.0 MB 11.1 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 22.0/150.0 MB 11.1 MB/s eta 0:00:12\n",
      "   ------ --------------------------------- 24.6/150.0 MB 11.2 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 27.0/150.0 MB 11.3 MB/s eta 0:00:11\n",
      "   ------- -------------------------------- 29.6/150.0 MB 11.5 MB/s eta 0:00:11\n",
      "   -------- ------------------------------- 33.0/150.0 MB 11.7 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 35.9/150.0 MB 11.9 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 38.8/150.0 MB 12.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 40.9/150.0 MB 12.0 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 44.0/150.0 MB 12.1 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 47.2/150.0 MB 12.2 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 50.3/150.0 MB 12.4 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 52.2/150.0 MB 12.2 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 56.9/150.0 MB 12.7 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 60.0/150.0 MB 12.8 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 62.9/150.0 MB 12.9 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 65.5/150.0 MB 12.9 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 68.7/150.0 MB 13.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 71.0/150.0 MB 13.0 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 74.4/150.0 MB 13.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 77.9/150.0 MB 13.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 81.0/150.0 MB 13.2 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 83.4/150.0 MB 13.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 83.9/150.0 MB 12.8 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 87.3/150.0 MB 12.9 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 90.4/150.0 MB 12.9 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 93.3/150.0 MB 13.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 95.9/150.0 MB 13.0 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 96.5/150.0 MB 12.9 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 98.6/150.0 MB 12.6 MB/s eta 0:00:05\n",
      "   -------------------------- ------------ 101.2/150.0 MB 12.6 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 104.3/150.0 MB 12.7 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 107.5/150.0 MB 12.8 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 110.9/150.0 MB 12.8 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 113.8/150.0 MB 12.8 MB/s eta 0:00:03\n",
      "   ------------------------------ -------- 116.9/150.0 MB 12.9 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 120.1/150.0 MB 13.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 123.5/150.0 MB 13.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 124.8/150.0 MB 13.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 127.4/150.0 MB 13.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 128.7/150.0 MB 12.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 132.1/150.0 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 135.0/150.0 MB 12.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 137.6/150.0 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 140.5/150.0 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 143.4/150.0 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 146.0/150.0 MB 12.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.2/150.0 MB 12.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 12.9 MB/s eta 0:00:01\n",
      "   --------------------------------------- 150.0/150.0 MB 12.6 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
